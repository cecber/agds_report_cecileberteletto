---
title: "land_cover_exercise"
output: html_document
---

```{r}
# install packages if not done (truc avec la loop l√†!!)
# Load required packages
library(dplyr)
library(tidyr)
library(parsnip)
library(workflows)
library(tune)
library(dials)
library(rsample)
library(caret)
library(readr)
```

# Exercise 8.3: Land-Use and Land-Cover Model Improvement
## PART 1: FOUR WAYS TO IMPROVE THE MODEL
1. INCREASE NUMBER OF SPECTRAL BANDS
 - Original uses only bands 1-4
 - Use all 7 MODIS bands for more spectral information
 - Add derived indices (NDVI, EVI, SAVI) as features

2. EXPAND HYPERPARAMETER TUNING
 - Original only tunes min_n and tree_depth with 3 combinations
 - Add learning rate (learn_rate) to tuning
 - Increase grid size to 20+ combinations for better optimization
 - Consider tuning additional parameters: loss_reduction, sample_size

3. INCREASE NUMBER OF TREES
 - Original uses only 50 trees
 - Test with 100-200 trees for potentially better ensemble performance
 - Balance between accuracy and computational cost

4. IMPROVE CROSS-VALIDATION STRATEGY
 - Original uses only 3-fold CV
 - Use 5 or 10-fold CV for more robust parameter selection
 - Consider spatial cross-validation to account for spatial autocorrelation

Additional improvements (bonus):
5. Feature engineering: Add temporal statistics (mean, sd, percentiles)
6. Handle class imbalance with stratified sampling or class weights
7. Ensemble multiple models (XGBoost + Random Forest + Neural Network)
8. Add more training data or use data augmentation

## PART 2: IMPLEMENTATION OF IMPROVEMENTS
in this section we downlead the data:
```{r}
# Set seed for reproducibility
set.seed(42)

# Download training and test data from Zenodo
cat("Downloading training and test data from Zenodo...\n")

# Create data directory
if (!dir.exists("data")) {
  dir.create("data")
}

# Download training data
if (!file.exists("data/training_data.rds")) {
  download.file(
    url = "https://zenodo.org/records/8298491/files/training_data.rds?download=1",
    destfile = "data/training_data.rds",
    mode = "wb"
  )
}

# Download test data
if (!file.exists("data/test_data.rds")) {
  download.file(
    url = "https://zenodo.org/records/8298491/files/test_data.rds?download=1",
    destfile = "data/test_data.rds",
    mode = "wb"
  )
}

cat("Loading data...\n")
training_data <- readr::read_rds("data/training_data.rds")
test_data <- readr::read_rds("data/test_data.rds")
```

in this section we then implement the first improvement: FEATURE ENGINEERING
this consist in adding derived spectral indices
```{r}

cat("Adding feature engineering (spectral indices)...\n")

# Function to calculate spectral indices
add_spectral_indices <- function(df) {
  # Get band column names (they contain "band")
  band_cols <- grep("band", names(df), value = TRUE)
  
  # For each date, calculate NDVI, EVI
  # MODIS bands: 1=Red, 2=NIR, 3=Blue, 7=MIR
  # We'll work with what we have (bands 1-4)
  
  # Get unique dates from column names
  dates <- unique(gsub(".*_(\\d{4}-\\d{2}-\\d{2})", "\\1", band_cols))
  
  for (date in dates) {
    # Get band columns for this date
    date_pattern <- paste0("_", date)
    date_bands <- grep(date_pattern, band_cols, value = TRUE)
    
    # Identify Red (band1) and NIR (band2) columns
    red_col <- grep("band1", date_bands, value = TRUE)[1]
    nir_col <- grep("band2", date_bands, value = TRUE)[1]
    
    if (length(red_col) > 0 && length(nir_col) > 0) {
      # Calculate NDVI: (NIR - Red) / (NIR + Red)
      ndvi_col_name <- paste0("NDVI_", date)
      df[[ndvi_col_name]] <- (df[[nir_col]] - df[[red_col]]) / 
                              (df[[nir_col]] + df[[red_col]] + 0.0001)
      
      # If we have blue band (band3), calculate EVI
      blue_col <- grep("band3", date_bands, value = TRUE)[1]
      if (length(blue_col) > 0) {
        # EVI = 2.5 * ((NIR - Red) / (NIR + 6*Red - 7.5*Blue + 1))
        evi_col_name <- paste0("EVI_", date)
        df[[evi_col_name]] <- 2.5 * ((df[[nir_col]] - df[[red_col]]) / 
                                     (df[[nir_col]] + 6*df[[red_col]] - 
                                      7.5*df[[blue_col]] + 1))
      }
    }
  }
  
  return(df)
}

# Add spectral indices to training and test data
training_data_enhanced <- add_spectral_indices(training_data)
test_data_enhanced <- add_spectral_indices(test_data)

# Prepare training data (remove ID columns)
train <- training_data_enhanced |>
  select(-c(pixelID, lat, lon))

# Prepare test data (keep pixelID for submission)
test <- test_data_enhanced
```

in this section we then implement the first improvement:EXPANDED HYPERPARAMETER TUNING:
```{r}
cat("Setting up improved model with expanded hyperparameter tuning...\n")

# Model specification with more parameters to tune and more trees
model_settings_improved <- parsnip::boost_tree(
  trees = 150,              # Increased from 50 to 150
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),       # Added learning rate to tuning
  loss_reduction = tune()    # Added loss reduction to tuning
) |>
  set_engine("xgboost") |>
  set_mode("classification")

# Create workflow
xgb_workflow_improved <- workflows::workflow() |>
  add_formula(as.factor(LC1) ~ .) |>
  add_model(model_settings_improved)
```

two further improvement methodes:
- BETTER CROSS-VALIDATION (5-fold instead of 3-fold)
- LARGER HYPERPARAMETER GRID
```{r}
cat("Setting up 5-fold cross-validation...\n")
# Create 5-fold cross-validation (increased from 3)
folds_improved <- rsample::vfold_cv(train, v = 5, strata = LC1)


cat("Creating larger hyperparameter grid...\n")
# Create a larger latin hypercube grid (20 combinations instead of 3)
hp_settings_improved <- dials::grid_latin_hypercube(
  min_n(range = c(5, 30)),
  tree_depth(range = c(4, 15)),
  learn_rate(range = c(-3, -0.5), trans = scales::log10_trans()),
  loss_reduction(range = c(-5, 1), trans = scales::log10_trans()),
  size = 20  # Increased from 3 to 20
)
cat("Hyperparameter grid preview:\n")
print(head(hp_settings_improved))
```

## Model training with implemented improvements 
```{r}

cat("\nStarting model training with improved settings...\n")
cat("This may take several minutes...\n")

# Tune the model with improved settings
xgb_results_improved <- tune::tune_grid(
  xgb_workflow_improved,
  resamples = folds_improved,
  grid = hp_settings_improved,
  control = tune::control_grid(
    save_pred = TRUE,
    verbose = TRUE
  ),
  metrics = yardstick::metric_set(
    yardstick::accuracy,
    yardstick::roc_auc,
    yardstick::kap
  )
)


# MODEL SELECTION AND FINALIZATION:

cat("\nSelecting best model based on ROC AUC...\n")

# View top models
show_best(xgb_results_improved, metric = "roc_auc", n = 5)

# Select best model
xgb_best_improved <- tune::select_best(
  xgb_results_improved,
  metric = "roc_auc"
)

cat("\nBest hyperparameters:\n")
print(xgb_best_improved)

# Finalize workflow with best parameters
xgb_best_hp_improved <- tune::finalize_workflow(
  xgb_workflow_improved,
  xgb_best_improved
)

# Fit final model on all training data
cat("\nFitting final model on all training data...\n")
xgb_final_model <- fit(xgb_best_hp_improved, data = train)


# MODEL EVALUATION (if labels are available in test data)

# GENERATE PREDICTIONS FOR LEADERBOARD SUBMISSION


cat("\nGenerating predictions for leaderboard submission...\n")

# Make predictions on test data
final_predictions <- predict(xgb_final_model, test)

# Prepare submission file
submission <- data.frame(
  lulc_class = final_predictions$.pred_class
)

# Save results
output_file <- "username_results.csv"
readr::write_csv(submission, output_file)

cat("\n=== SUBMISSION FILE CREATED ===\n")
cat("File saved as:", output_file, "\n")
cat("\nTo submit to the leaderboard:\n")
cat("1. Fork the repository 'geco-bern/agds2_course'\n")
cat("2. Add your results as 'data/leaderboard/fall_2025/username_results.csv'\n")
cat("   (replace 'username' with your GitHub username)\n")
cat("3. Commit and push your changes\n")
cat("4. Open a pull request to the main repository\n")
```

now we compare the new model with the baseline one
```{r}

# COMPARISON WITH BASELINE MODEL (Optional)

cat("\n=== IMPROVEMENTS IMPLEMENTED ===\n")
cat("1. Feature Engineering: Added NDVI and EVI spectral indices\n")
cat("2. Expanded Hyperparameter Tuning:\n")
cat("   - Added learn_rate and loss_reduction to tuning\n")
cat("   - Increased grid size from 3 to 20 combinations\n")
cat("3. Increased number of trees from 50 to 150\n")
cat("4. Improved cross-validation from 3-fold to 5-fold\n")

cat("\n=== ADDITIONAL IMPROVEMENT SUGGESTIONS ===\n")
cat("- Use all 7 MODIS spectral bands (currently only using 4)\n")
cat("- Add temporal statistics (min, max, range, percentiles of time series)\n")
cat("- Implement spatial cross-validation for spatial autocorrelation\n")
cat("- Ensemble multiple algorithms (Random Forest, Neural Networks)\n")
cat("- Use class weights to handle class imbalance\n")
cat("- Add more training data or data augmentation techniques\n")

cat("\n=== SCRIPT COMPLETED ===\n")
```

